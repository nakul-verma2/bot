# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-MSLfvwQq8AsoA0Pl4MVFsHy8KB82blU
"""

from google.colab import files
files.upload()  # Upload kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!pip install kagglehub pandas langchain langchain-community sentence-transformers faiss-cpu transformers torch

import kagglehub
import pandas as pd
import os
import zipfile
from langchain.schema import Document

def download_dataset():
    """
    Download the Simple Dialogs for Chatbot dataset using kagglehub and handle potential zip files.

    Returns:
        str: Path to the dialogs.txt file.
    """
    try:
        # Download the latest version of the dataset
        dataset_path = kagglehub.dataset_download("grafstor/simple-dialogs-for-chatbot")
        print("Path to dataset files:", dataset_path)

        # List all files in the dataset directory
        print("Files in dataset directory:")
        for root, _, files in os.walk(dataset_path):
            for file in files:
                print(f" - {file}")
                # Check for zip files and extract them
                if file.endswith('.zip'):
                    zip_path = os.path.join(root, file)
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(root)
                        print(f"Extracted {zip_path}")

        # Search for dialogs.txt (case-insensitive)
        for root, _, files in os.walk(dataset_path):
            for file in files:
                if file.lower() == "dialogs.txt":
                    return os.path.join(root, file)

        raise FileNotFoundError("dialogs.txt not found in the downloaded dataset. Check listed files above.")

    except Exception as e:
        print(f"Error downloading dataset: {e}")
        raise

def load_dataset(file_path: str) -> list[Document]:
    """
    Load the Simple Dialogs for Chatbot TXT dataset and convert it to LangChain Document format.

    Args:
        file_path (str): Path to the TXT file.

    Returns:
        list[Document]: List of LangChain Document objects.
    """
    try:
        # Read the text file
        documents = []
        with open(file_path, 'r', encoding='utf-8') as file:
            # Assuming tab-separated format: Input\tResponse
            for line in file:
                # Skip empty lines
                if not line.strip():
                    continue
                # Split line into question and response (adjust delimiter if needed)
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    question, response = parts[0], parts[1]
                    if question and response:  # Ensure neither is empty
                        documents.append(
                            Document(
                                page_content=str(response),
                                metadata={"question": str(question)}
                            )
                        )

        if not documents:
            raise ValueError("No valid question-response pairs found in dialogs.txt")

        return documents

    except Exception as e:
        print(f"Error loading dataset: {e}")
        raise

# Download and load dataset
try:
    txt_path = download_dataset()
    documents = load_dataset(txt_path)

    # Print first few documents to verify
    for doc in documents[:3]:
        print(f"Content: {doc.page_content[:100]}...")
        print(f"Metadata: {doc.metadata}\n")
except Exception as e:
    print(f"Error in main execution: {e}")

!pip install kagglehub pandas langchain langchain-community sentence-transformers faiss-cpu transformers torch

import kagglehub
import pandas as pd
import os
import zipfile
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

def download_dataset():
    """
    Download the Simple Dialogs for Chatbot dataset using kagglehub and handle potential zip files.

    Returns:
        str: Path to the dialogs.txt file.
    """
    try:
        dataset_path = kagglehub.dataset_download("grafstor/simple-dialogs-for-chatbot")
        print("Path to dataset files:", dataset_path)

        print("Files in dataset directory:")
        for root, _, files in os.walk(dataset_path):
            for file in files:
                print(f" - {file}")
                if file.endswith('.zip'):
                    zip_path = os.path.join(root, file)
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(root)
                        print(f"Extracted {zip_path}")

        for root, _, files in os.walk(dataset_path):
            for file in files:
                if file.lower() == "dialogs.txt":
                    return os.path.join(root, file)

        raise FileNotFoundError("dialogs.txt not found in the downloaded dataset. Check listed files above.")

    except Exception as e:
        print(f"Error downloading dataset: {e}")
        raise

def load_dataset(file_path: str) -> list[Document]:
    """
    Load the Simple Dialogs for Chatbot TXT dataset and convert it to LangChain Document format.

    Args:
        file_path (str): Path to the TXT file.

    Returns:
        list[Document]: List of LangChain Document objects.
    """
    try:
        documents = []
        with open(file_path, 'r', encoding='utf-8') as file:
            for line in file:
                if not line.strip():
                    continue
                parts = line.strip().split('\t')
                if len(parts) >= 2:
                    question, response = parts[0], parts[1]
                    if question and response:
                        documents.append(
                            Document(
                                page_content=str(response),
                                metadata={"question": str(question)}
                            )
                        )

        if not documents:
            raise ValueError("No valid question-response pairs found in dialogs.txt")

        return documents

    except Exception as e:
        print(f"Error loading dataset: {e}")
        raise

def setup_rag_pipeline(documents):
    """
    Set up a RAG pipeline using LangChain with FAISS vector store and a Hugging Face LLM.

    Args:
        documents (list[Document]): List of LangChain Document objects.

    Returns:
        RetrievalQA: Configured RAG chain for question answering.
    """
    try:
        # Initialize embeddings
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

        # Create FAISS vector store from documents
        vector_store = FAISS.from_documents(documents, embeddings)

        # Initialize the retriever
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})  # Retrieve top 3 documents

        # Set up the LLM (using Hugging Face's GPT-2 as an example; replace with a better model if needed)
        model_name = "gpt2"  # Consider using "distilgpt2" or another lightweight model for Colab
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=50,
            truncation=True
        )
        llm = HuggingFacePipeline(pipeline=text_generation_pipeline)

        # Set up the RAG pipeline
        rag_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",  # Use retrieved documents directly
            retriever=retriever,
            return_source_documents=True
        )

        return rag_chain

    except Exception as e:
        print(f"Error setting up RAG pipeline: {e}")
        raise

# Download and load dataset
try:
    txt_path = download_dataset()
    documents = load_dataset(txt_path)

    # Print first few documents to verify
    for doc in documents[:3]:
        print(f"Content: {doc.page_content[:100]}...")
        print(f"Metadata: {doc.metadata}\n")

    # Set up RAG pipeline
    rag_chain = setup_rag_pipeline(documents)

    # Test the RAG pipeline with a sample question
    sample_question = "How are you doing?"
    result = rag_chain({"query": sample_question})
    print(f"Question: {sample_question}")
    print(f"Answer: {result['result']}")
    print(f"Source Documents: {[doc.page_content for doc in result['source_documents']]}")

except Exception as e:
    print(f"Error in main execution: {e}")

from huggingface_hub import login
login("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")

def run_chatbot(rag_chain):
    """
    Run an interactive chatbot using the RAG pipeline.

    Args:
        rag_chain: Configured RetrievalQA chain.
    """
    print("Chatbot is running. Type 'exit' to stop.")
    while True:
        question = input("Enter your question: ")
        if question.lower() == 'exit':
            print("Exiting chatbot.")
            break
        try:
            result = rag_chain({"query": question})
            print(f"Answer: {result['result']}")
            print("Source Documents:")
            for doc in result['source_documents']:
                print(f" - {doc.page_content} (Question: {doc.metadata['question']})")
        except Exception as e:
            print(f"Error answering question: {e}")

# Add to main execution block
try:
    txt_path = download_dataset()
    documents = load_dataset(txt_path)
    for doc in documents[:3]:
        print(f"Content: {doc.page_content[:100]}...")
        print(f"Metadata: {doc.metadata}\n")

    rag_chain = setup_rag_pipeline(documents)
    run_chatbot(rag_chain)
except Exception as e:
    print(f"Error in main execution: {e}")